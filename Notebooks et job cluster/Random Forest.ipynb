{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modèle Random Forest\n",
    "Ce modèle a été moins développé que les autres mais le code vous est accessible\n",
    "## Initialisation de PySpark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext created\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import HashingTF, CountVectorizer, Tokenizer, IDF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.master('local[16]').getOrCreate()\n",
    "\n",
    "print(\"SparkContext created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Récupération des données et répartitions des classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class repartitions\n",
      "+-----+------+\n",
      "|class| count|\n",
      "+-----+------+\n",
      "|    0|800000|\n",
      "|    4|800000|\n",
      "+-----+------+\n",
      "\n",
      "Dataset schema :\n",
      "root\n",
      " |-- class: integer (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = '../resources/training_noemoticon.csv'\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)])\n",
    "\n",
    "df = spark.read.options(inferSchema=True,\n",
    "                        ignoreLeadingWhiteSpace=True,\n",
    "                        schema=schema).csv(filename)\n",
    "df = df.dropna()\n",
    "\n",
    "official_col = ['class', 'tweet_id', 'date', 'query', 'username', 'content']\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    df = df.withColumnRenamed(column, official_col[i])\n",
    "\n",
    "df = df.select(\"class\",\"content\")\n",
    "\n",
    "print(\"Dataset class repartitions\")\n",
    "gr = df.groupBy(\"class\").count()\n",
    "gr.show()\n",
    "\n",
    "print(\"Dataset schema :\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Traitement des features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chooses CountVectorize or HashingTF\n",
    "cVec = False\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "if cVec:\n",
    "    cv = CountVectorizer(inputCol=\"words\", outputCol=\"r_features\")\n",
    "    df = cv.fit(df)\n",
    "else:\n",
    "    hashtf = HashingTF(inputCol=\"words\", outputCol=\"r_features\")\n",
    "    df = hashtf.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"r_features\", outputCol=\"features\")\n",
    "\n",
    "step = idf.fit(df)\n",
    "df = step.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partage entrainement et test (80/20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "train, test = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrainement et évaluation du modèle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\", maxDepth=10)\n",
    "\n",
    "model = rf.fit(train)\n",
    "\n",
    "# run on test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# evaluate\n",
    "print()\n",
    "print(\"Accuracy = \", evaluator.evaluate(predictions))\n",
    "\n",
    "predictions.groupBy('class','prediction').count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "69132f190f4b668d460737ebcde5689e94028e0059afe55fa12d464dbc238f4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
